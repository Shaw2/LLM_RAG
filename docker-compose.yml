services:
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.14
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - service_network

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9000:9000"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - service_network

  standalone:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "standalone"]
    security_opt:
    - seccomp:unconfined
    environment:
      MINIO_REGION: us-east-1
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      LOG_LEVEL: warn
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"
    networks:
      service_network:
        ipv4_address: 172.19.0.6


  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    build:
      context: ./ollama
      dockerfile: Dockerfile
    ports:
      - "11434:11434"
    volumes:
      - ./llama_models:/models
    environment:
      - OLLAMA_API_PORT=11434
      - OLLAMA_HOST=0.0.0.0:11434
      - CUDA_VISIBLE_DEVICES=0  # GPU 0번만 사용
      - TF_FORCE_GPU_ALLOW_GROWTH=true  # 메모리 동적 할당
    networks:
      - service_network
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
      
  langchain:
    build:
      context: ./langchain  # 현재 디렉토리를 컨텍스트로 사용
      dockerfile: Dockerfile
    container_name: langchain
    depends_on:
      - ollama
    ports:
      - "8001:8001"
    volumes:
      - ./langchain:/app/langchain
    environment:
      - ollama_IP=172.19.0.6
      # - ollama_IP=host.docker.internal
      - CUDA_VISIBLE_DEVICES=0  # GPU 0번만 사용
      - TF_FORCE_GPU_ALLOW_GROWTH=true  # 메모리 동적 할당
    networks:
      - service_network
    command: uvicorn main:app --host 0.0.0.0 --port 8001 --reload  # Uvicorn 실행 명령어
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

networks:
  service_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.19.0.0/16

volumes:
  etcd-data:
  minio-data:
